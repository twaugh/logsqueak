# LLM API Contracts

**Date**: 2025-11-04

**Feature**: 002-logsqueak-spec

## Overview

This document defines the expected request and response formats for LLM API interactions. The application supports both OpenAI-compatible APIs (OpenAI, Ollama with `/v1/chat/completions` endpoint) and Ollama's native API (`/api/chat`).

All LLM requests use **streaming responses** with **NDJSON (Newline-Delimited JSON)** format for incremental results. Each response is a single JSON object per line, not wrapped in SSE or arrays.

**Why NDJSON?**
- Simpler than SSE (no `data:` prefix parsing)
- Each line is complete JSON object (easier to log and debug)
- Error isolation: bad JSON on one line doesn't corrupt stream
- Perfect fit with httpx.aiter_lines()
- Compatible with both OpenAI and Ollama APIs

### Template Variables

Prompt templates use the following variables:

- `{indent_style}`: Description of detected indentation (e.g., "2 spaces per level", "tab character per level")
- `{journal_content}`: Full journal file content
- `{xml_blocks}`: XML-wrapped blocks with context
- `{xml_formatted_content}`: XML-wrapped knowledge blocks and candidate pages

### NDJSON Parsing Pattern

```python
import httpx
import json

async def parse_ndjson_stream(url, payload):
    async with httpx.AsyncClient(timeout=60.0) as client:
        async with client.stream("POST", url, json=payload) as response:
            async for line in response.aiter_lines():
                if line.strip():  # Skip empty lines
                    try:
                        data = json.loads(line)
                        yield data
                    except json.JSONDecodeError as e:
                        logger.error("malformed_json_line", line=line, error=str(e))
                        continue  # Skip bad line, process remaining stream
```

**Error Handling**:
- **Malformed JSON**: Log error, skip line, continue with remaining stream
- **Network timeout**: Preserve partial results, trigger retry logic
- **Connection closed**: Treat as end-of-stream (no explicit marker needed)

---

## Phase 1: Knowledge Block Classification

### Request

**Endpoint**: Configured via `config.llm.endpoint`

- OpenAI-compatible: `POST {endpoint}/chat/completions`
- Ollama native: `POST {endpoint}/api/chat`

**Headers**:

```
Authorization: Bearer {config.llm.api_key}
Content-Type: application/json

```

**Request Body** (OpenAI-compatible):

```json
{
  "model": "{config.llm.model}",
  "messages": [
    {
      "role": "system",
      "content": "You are an AI assistant that identifies lasting knowledge in Logseq journal entries. Logseq uses markdown with indented bullets ({indent_style}). Blocks can have continuation lines (indented text without bullets) and properties (key:: value). Each block has an id:: property for identification.\n\nAnalyze the journal entry and return ONLY blocks that contain lasting insights worth preserving (skip temporal events/tasks/activities). When a child block contains knowledge that depends on parent context, return ONLY the specific child block ID - not all parent blocks. Output one JSON object per line (NDJSON format), not a JSON array. Each object must have: block_id (string), confidence (float 0-1), reason (string explaining why this is knowledge worth preserving)."
    },
    {
      "role": "user",
      "content": "Analyze this Logseq journal entry:\n\n{journal_content}"
    }
  ],
  "stream": true,
  "temperature": 0.3
}

```

**Request Body** (Ollama native):

```json
{
  "model": "{config.llm.model}",
  "messages": [...same as above...],
  "stream": true,
  "options": {
    "temperature": 0.3,
    "num_ctx": "{config.llm.num_ctx}"
  }
}

```

**Journal Content Example**:

```markdown
- Morning standup with team
  id:: 65a1b2c3
  - Discussed project timeline
    id:: 65a1b2c4
  - Python's asyncio.Queue is thread-safe
    id:: abc123
    This is important for background tasks in Textual
- Had lunch with Sarah
  id:: def456
  Discussed project roadmap
- Discovered ChromaDB embedding dimensions
  id:: ghi789
  Default model uses 384-dimensional embeddings
  Can be configured via collection settings
```

**Note**: The LLM receives the entire journal file content with all `id::` properties (explicit UUIDs or content hashes generated by the parser).

---

### Response (Streaming)

**Format**: NDJSON (Newline-Delimited JSON)

Each line is a complete JSON object representing one identified knowledge block. The LLM should ONLY output blocks it identifies as knowledge (not every input block).

**Expected output** (one JSON object per line):

```
{"block_id": "abc123", "confidence": 0.85, "reason": "Technical insight about Python's asyncio.Queue being thread-safe - evergreen knowledge about concurrency patterns."}
{"block_id": "ghi789", "confidence": 0.92, "reason": "Documents ChromaDB embedding dimensions and configuration - useful technical reference."}
```

**Important**: LLM should return `abc123` (the specific child block with knowledge), NOT `65a1b2c3` (the parent "Morning standup" block). The child block ID is sufficient because the system will retrieve full parent context during Phase 2.

**Important**:
- No JSON array wrapping (not `[{...}, {...}]`)
- No SSE format (not `data: {...}`)
- One JSON object per line (NDJSON)
- Only knowledge blocks returned (activity blocks omitted entirely)

**Stream End**: Connection closes (no special end marker needed)

**Error Response** (non-streaming):

```json
{
  "error": {
    "message": "Invalid API key",
    "type": "invalid_request_error"
  }
}

```

---

## Phase 2: Content Rewording

### Request

**Endpoint**: Same as Phase 1

**Request Body** (OpenAI-compatible):

```json
{
  "model": "{config.llm.model}",
  "messages": [
    {
      "role": "system",
      "content": "You are an AI assistant that transforms journal-style content into evergreen knowledge. You will receive blocks wrapped in XML, each with full parent context including page properties. The blocks are properly indented following Logseq's indentation ({indent_style}).\n\nRemove temporal context (dates, 'today', 'yesterday'), convert first-person to third-person or neutral, and make the content timeless. Preserve technical details and insights. Use parent context to understand meaning, but only reword the specific block (the deepest/last block in each context hierarchy).\n\nIMPORTANT: Resolve all pronouns and references using parent context. If the block says 'this' or 'it', replace with the actual subject from parent context. Example: if parent says 'Tried Textual framework' and child says 'This is Python-specific', reword as 'The Textual framework is Python-specific'.\n\nOutput one JSON object per line (NDJSON format), not a JSON array. Each object must have: block_id (string), reworded_content (string)."
    },
    {
      "role": "user",
      "content": "{xml_blocks}"
    }
  ],
  "stream": true,
  "temperature": 0.5
}

```

**XML Blocks Example**:

```xml
<blocks>
  <block id="abc123">
area:: [[TUI]]
tags:: #work

- Morning standup with team
  - Discussed project timeline
  - Learned today that Python's asyncio.Queue is thread-safe
    This is important for background tasks in Textual
    I should remember this when designing concurrent systems
    id:: abc123
  </block>

  <block id="ghi789">
tags:: #databases

- Discovered ChromaDB embedding dimensions
  Default model uses 384-dimensional embeddings
  Can be configured via collection settings
  id:: ghi789
  </block>
</blocks>
```

**Note**:
- Each `<block>` shows page properties at the top (if present)
- Full parent hierarchy with proper indentation (using detected `{indent_style}`)
- The deepest/last block in each hierarchy is the one to reword
- The `id::` property identifies which block to reword
- Parent blocks and page properties provide context but are not reworded
- Example shows 2-space indentation, but actual indentation matches the graph

---

### Response (Streaming)

**Format**: NDJSON (Newline-Delimited JSON)

Each line is a complete JSON object representing one reworded knowledge block.

**Expected output** (one JSON object per line):

```
{"block_id": "abc123", "reworded_content": "Python's asyncio.Queue is thread-safe, making it suitable for communication between background tasks and the main thread in Textual applications. This property is important when designing concurrent systems."}
{"block_id": "ghi789", "reworded_content": "ChromaDB's default embedding model uses 384-dimensional embeddings, which can be configured via collection settings."}
```

**Note**: LLM uses parent context ("Morning standup with team") and page properties to understand meaning, but only rewords the deepest block's content.

**Important**:
- No JSON array wrapping
- No SSE format
- One JSON object per line (NDJSON)

**Stream End**: Connection closes

---

## Phase 3: Integration Decisions

### Request

**Endpoint**: Same as Phase 1

**Request Body** (OpenAI-compatible):

```json
{
  "model": "{config.llm.model}",
  "messages": [
    {
      "role": "system",
      "content": "You are an AI assistant that decides where to integrate knowledge into a Logseq knowledge base. You will receive knowledge blocks with their original journal context and candidate target pages with their structure.\n\nRELEVANCE FILTERING:\n- Only output decisions where confidence ≥ 0.30 (30%)\n- Only output decisions where semantic connection is clear and actionable\n- Maximum 2 decisions per (knowledge_block, target_page) pair - choose the 2 best integration points if multiple locations are relevant\n- If a candidate page is not relevant, omit it entirely (do not output a decision for it)\n- If NO candidate pages meet the threshold, output nothing (empty stream)\n\nDUPLICATE DETECTION:\n- FIRST check if similar/identical knowledge already exists in each candidate page\n- If duplicate found, use action='skip_exists' with target_block_id pointing to existing block\n- For skip_exists: provide reasoning explaining why it's a duplicate\n- For skip_exists: set target_block_title to describe location (e.g., \"Already exists at 'Section Name'\")\n\nFor each relevant NEW integration, choose the best action:\n- 'add_section': Create new top-level section on the page\n- 'add_under': Add as child under an existing block (provide target_block_id)\n- 'replace': Replace an existing block's content (provide target_block_id)\n- 'skip_exists': Knowledge already exists in target page (provide target_block_id of existing block)\n\nOutput one JSON object per line (NDJSON format), not a JSON array. Each object must have: knowledge_block_id (string), target_page (string), action (string), target_block_id (string or null), target_block_title (string or null), confidence (float 0-1), reasoning (string explaining why this integration makes sense or why it's a duplicate)."
    },
    {
      "role": "user",
      "content": "{xml_formatted_content}"
    }
  ],
  "stream": true,
  "temperature": 0.4
}

```

**XML-Formatted Content Example**:

```xml
<knowledge_blocks>
  <block id="abc123">
    <original_journal_context>
- Morning standup with team
  - Discussed project timeline
  - Learned today that Python's asyncio.Queue is thread-safe
    This is important for background tasks in Textual
    I should remember this when designing concurrent systems
    id:: abc123
    </original_journal_context>
    <refined_content>Python's asyncio.Queue is thread-safe, making it suitable for communication between background tasks and the main thread in Textual applications. This property is important when designing concurrent systems.</refined_content>
  </block>
</knowledge_blocks>

<candidate_pages>
  <page name="Python/Concurrency">
    <page_properties>
      tags:: #python #concurrency
      created:: 2024-01-15
    </page_properties>
    <existing_blocks>
      <block>
- Threading vs Asyncio
  id:: threading-section-456
  - Threading uses locks for shared state
    id:: threading-detail-789
  - Asyncio uses coroutines
    id:: asyncio-detail-012
      </block>
      <block>
- Common Patterns
  id:: pattern-section-123
      </block>
    </existing_blocks>
  </page>

  <page name="Textual/Architecture">
    <page_properties>
      tags:: #textual #architecture
    </page_properties>
    <existing_blocks>
      <block>
- Widget Lifecycle
  id:: lifecycle-section-345
      </block>
      <block>
- Message Passing
  id:: message-section-456
  - Worker communication patterns
    id:: worker-comm-567
      </block>
    </existing_blocks>
  </page>
</candidate_pages>
```

**Note**:
- `<original_journal_context>`: Full parent context from journal (properly indented)
- `<refined_content>`: The reworded evergreen version (from Phase 2)
- `<page_properties>`: Target page properties (for context, not for modification)
- `<existing_blocks>`: Each block shows full context with proper indentation and `id::` properties

---

### Response (Streaming)

**Format**: NDJSON (Newline-Delimited JSON)

Each line is a complete JSON object representing one integration decision (one knowledge block + one candidate page).

**Expected output** (one JSON object per line):

```
{"knowledge_block_id": "abc123", "target_page": "Python/Concurrency", "action": "add_under", "target_block_id": "pattern-section-123", "target_block_title": "Common Patterns", "confidence": 0.92, "reasoning": "This knowledge about asyncio.Queue's thread-safety is a common pattern in Python concurrency. Adding it under the 'Common Patterns' section provides clear context and makes it discoverable."}
```

**Example with duplicate detection**:

```
{"knowledge_block_id": "abc123", "target_page": "Python/Concurrency", "action": "skip_exists", "target_block_id": "asyncio-queue-789", "target_block_title": "Already exists at 'Asyncio Patterns'", "confidence": 0.95, "reasoning": "This knowledge about asyncio.Queue's thread-safety already exists in the target page under 'Asyncio Patterns' with nearly identical wording."}
```

**Important**:
- No JSON array wrapping
- No SSE format
- One JSON object per line (NDJSON)
- Only return relevant integrations (omit pages that are not good fits)
- Use `action="skip_exists"` when duplicate content is found in target page
- For skip_exists decisions, system will filter them out by default but show count in summary
- In the first example, `Textual/Architecture` is omitted because it's not a strong fit

**Stream End**: Connection closes

### Relevance Filtering (FR-032a, FR-032b)

**Filtering Strategy**: Prompt-based (LLM performs filtering before outputting decisions)

The system prompt instructs the LLM to apply these filters:

1. **Confidence threshold**: Only output decisions with confidence ≥ 30%
2. **Semantic relevance**: Only output decisions where there's a clear, actionable connection between the knowledge block and target page
3. **Max decisions per (knowledge_block, target_page) pair**: At most 2 decisions per pair
   - If multiple integration points on same page are relevant, LLM selects the 2 best locations
   - Rationale: Prevents overwhelming user with too many options for same page

**Example Scenario**:
- RAG search returns 10 candidate pages
- LLM evaluates all 10 pages
- Only 2 pages have strong semantic relevance (confidence ≥ 30%)
- One page has 3 potential integration points (LLM returns only the 2 best)
- **Result**: 3 IntegrationDecisionChunk objects total (2 from first page, 1 from second page)

**Edge Case - No Relevant Pages**:
- If no candidate pages meet the confidence/relevance threshold
- LLM outputs empty stream (connection closes with no chunks)
- Client handles via FR-053 ("No relevant pages found" message)

**Rationale for Prompt-Based Filtering**:
- Simpler than client-side filtering (no duplicate logic)
- LLM has full semantic context to judge relevance
- User sees only actionable decisions (better UX)
- Aligns with Constitution Principle III (Simplicity)

---

## Error Handling

### Transient Errors (Auto-Retry)

These errors trigger automatic retry (once, 2s delay):

- `httpx.ConnectError` (connection refused, network unreachable)
- `httpx.ConnectTimeout` (connection timeout)
- `httpx.ReadTimeout` (read timeout during streaming)

### Permanent Errors (Show Error, Prompt User)

These errors show error message and prompt user for action:

- `401 Unauthorized` (invalid API key)
- `429 Too Many Requests` (rate limit exceeded)
- `500 Internal Server Error` (LLM service error)
- `503 Service Unavailable` (LLM service down)
- Malformed JSON in streaming response

**Error Display Format**:

```
Error: LLM request failed
Reason: Invalid API key (401 Unauthorized)
Suggested Action: Check llm.api_key in ~/.config/logsqueak/config.yaml

Options: [Retry] [Skip] [Cancel]

```

### Retry Logic

**Automatic retry pattern** (retry once, 2-second delay):

```python
from asyncio import sleep

async def retry_request(func, max_retries=1, delay=2.0):
    """Retry async function once on transient errors."""
    try:
        return await func()
    except (httpx.ConnectError, httpx.ReadTimeout, httpx.ConnectTimeout) as e:
        logger.warning("llm_request_retry", error=str(e), delay=delay)
        if max_retries > 0:
            await sleep(delay)
            return await func()
        raise
```

**After retry fails**: Prompt user with options (Retry, Skip, Cancel)

**Partial Results**: Preserve all successfully parsed chunks before network failure. Never discard partial state on network errors.

---

## Pydantic Chunk Models

All streaming responses are parsed into type-safe Pydantic models.

### Phase 1: KnowledgeClassificationChunk

```python
from pydantic import BaseModel, Field

class KnowledgeClassificationChunk(BaseModel):
    """Single classification result from NDJSON stream."""

    block_id: str = Field(..., description="Block identifier from input")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confidence score (0.0-1.0)")
    reason: str = Field(..., description="Explanation for why this is knowledge")
```

### Phase 2: ContentRewordingChunk

```python
class ContentRewordingChunk(BaseModel):
    """Single rewording result from NDJSON stream."""

    block_id: str = Field(..., description="Block identifier from input")
    reworded_content: str = Field(..., min_length=1, description="Reworded content with temporal context removed")
```

### Phase 3: IntegrationDecisionChunk

```python
from typing import Literal, Optional

class IntegrationDecisionChunk(BaseModel):
    """Single integration decision from NDJSON stream."""

    knowledge_block_id: str = Field(..., description="Block ID of the knowledge being integrated")
    target_page: str = Field(..., description="Target page name (hierarchical pages use '/' separator)")
    action: Literal["add_section", "add_under", "replace", "skip_exists"] = Field(..., description="Type of integration action")
    target_block_id: Optional[str] = Field(default=None, description="Target block ID for 'add_under', 'replace', or 'skip_exists' actions")
    target_block_title: Optional[str] = Field(default=None, description="Human-readable title of target block (for display)")
    confidence: float = Field(..., ge=0.0, le=1.0, description="LLM's confidence score for this integration (0.0-1.0)")
    reasoning: str = Field(..., description="LLM's explanation for this integration decision or duplicate detection")
```

**Note**: Phase 3 batching (FR-032): System waits for all decisions for a given knowledge block to arrive before displaying. Application must buffer all decisions with matching `knowledge_block_id`.

---

## Timeout Configuration

All requests use these timeout values (httpx.Timeout):

- `connect`: 10 seconds (initial connection)
- `read`: 60 seconds (per chunk during streaming)
- `write`: 10 seconds
- `pool`: 10 seconds

**Rationale**: 60s read timeout allows for slow streaming responses from resource-constrained local models (e.g., Ollama on CPU).

---

## Request Logging

All LLM requests must be logged for debugging. Log format:

```json
{
  "event": "llm_request_started",
  "timestamp": "2025-11-04T10:30:45.123Z",
  "request_id": "uuid-here",
  "phase": "classification",
  "model": "gpt-4-turbo-preview",
  "endpoint": "https://api.openai.com/v1/chat/completions",
  "num_blocks": 15,
  "messages": [...full messages array...]
}

```

Streaming chunks logged as:

```json
{
  "event": "llm_response_chunk",
  "timestamp": "2025-11-04T10:30:47.456Z",
  "request_id": "uuid-here",
  "chunk_num": 3,
  "data": {...parsed JSON...}
}

```

Request completion logged as:

```json
{
  "event": "llm_request_completed",
  "timestamp": "2025-11-04T10:31:02.789Z",
  "request_id": "uuid-here",
  "total_chunks": 15,
  "duration_ms": 17666
}

```

---

## Model Compatibility

### OpenAI API

- Endpoint: `https://api.openai.com/v1/chat/completions`
- Recommended models: `gpt-4-turbo-preview`, `gpt-3.5-turbo`
- **Streaming format**: OpenAI returns SSE (Server-Sent Events) with `data:` prefix
- **Client handling**: Strip `data:` prefix and parse as NDJSON
- **Note**: OpenAI wraps content in SSE, but the actual LLM response should be NDJSON (one JSON object per line in the content)

### Ollama API

- Endpoint: `http://localhost:11434/v1/chat/completions` (OpenAI-compatible)
- Alternative: `http://localhost:11434/api/chat` (native)
- Recommended models: `llama2`, `mistral`, `mixtral`
- **Streaming format**: Native NDJSON (newline-delimited JSON)
- **Client handling**: Parse each line as JSON directly
- Extra config: `num_ctx` controls context window (VRAM usage)

### Other OpenAI-Compatible Services

- Anthropic Claude (via proxy)
- Azure OpenAI
- Local models via text-generation-webui
- Any service implementing OpenAI chat completions API

---

## Summary

All LLM interactions follow these principles:

1. **Streaming-first**: All responses arrive incrementally via NDJSON
2. **NDJSON format**: LLM outputs one JSON object per line (not JSON arrays, not SSE-wrapped)
3. **Selective output**: Phase 1 only returns identified knowledge blocks (not all blocks)
4. **Context-aware prompts**: Full parent context and page properties provided via XML
5. **Pronoun resolution**: Phase 2 explicitly resolves references ("this" → actual subject)
6. **Type-safe parsing**: Pydantic chunk models validate each streamed result
7. **Error isolation**: Bad JSON on one line doesn't corrupt stream
8. **Retryable**: Transient failures automatically retry once (2s delay)
9. **Partial results**: Network failures preserve successfully parsed chunks
10. **Logged**: All requests, chunks, and errors logged for debugging
11. **Configurable**: Endpoint, model, and parameters from config file
12. **Dynamic indentation**: Prompts use detected `{indent_style}` from graph

### NDJSON vs SSE Clarification

- **What the LLM produces**: NDJSON (one JSON object per line in response content)
- **How it's transmitted**: May be wrapped in SSE by some APIs (OpenAI), or raw NDJSON (Ollama native)
- **What the client parses**: Always NDJSON after unwrapping any SSE envelope

This contract ensures consistent, debuggable, and resilient LLM interactions across all phases of the TUI application.
