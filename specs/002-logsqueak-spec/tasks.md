# Tasks: Logsqueak - Interactive TUI for Knowledge Extraction

**Input**: Design documents from `/specs/002-logsqueak-spec/`

**Prerequisites**: plan.md (complete), spec.md (complete), research.md (complete), data-model.md (complete), contracts/ (complete)

**Tests**: Tests are INCLUDED in this task breakdown following user requirement to "use pilot for testing TUI features" and "run tests to verify code actually works"

**Organization**: Tasks are grouped by user story to enable independent implementation and testing. Each foundational component includes tests that must pass before proceeding.

**User Instruction**: "Proceed cautiously, and use pilot for testing TUI features. Start with well-tested foundational components and let me verify them before continuing. Don't just write tests -- also run them to verify the code you've written actually works."

## Format: `[ID] [P?] [Story] Description`

- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (e.g., US1, US2, US3)
- Include exact file paths in descriptions

## Path Conventions

- Single project structure at repository root
- Source: `src/logsqueak/`
- Tests: `tests/`
- Existing parser: `src/logseq-outline-parser/` (DO NOT MODIFY)

---

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: Project initialization and basic structure

- [x] T001 Update pyproject.toml with TUI dependencies (textual, httpx, pydantic, click, chromadb, sentence-transformers, markdown-it-py, pyyaml, structlog, pytest-textual-snapshot)
- [x] T002 Create main package structure: src/logsqueak/{__init__.py,cli.py,config.py}
- [x] T003 [P] Create models package structure: src/logsqueak/models/{__init__.py,block_state.py,edited_content.py,integration_decision.py,background_task.py}
- [x] T004 [P] Create services package structure: src/logsqueak/services/{__init__.py,llm_client.py,page_indexer.py,rag_search.py,file_monitor.py}
- [x] T005 [P] Create TUI package structure: src/logsqueak/tui/{__init__.py,app.py,screens/__init__.py,widgets/__init__.py}
- [x] T006 [P] Create utils package structure: src/logsqueak/utils/{__init__.py,logging.py,ids.py}
- [x] T007 [P] Create test directory structure: tests/{unit,integration,ui}/{__init__.py}
- [x] T008 Configure structlog for JSON logging to ~/.cache/logsqueak/logs/logsqueak.log
- [x] T009 Create CLI entry point with Click in src/logsqueak/cli.py (basic 'extract' command placeholder)

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: Core infrastructure that MUST be complete and TESTED before ANY user story can be implemented

**âš ï¸ CRITICAL**: No user story work can begin until this phase is complete and all tests pass

### Configuration Management

- [x] T010 [P] Implement LLMConfig Pydantic model in src/logsqueak/models/config.py
- [x] T011 [P] Implement LogseqConfig Pydantic model in src/logsqueak/models/config.py
- [x] T012 [P] Implement RAGConfig Pydantic model in src/logsqueak/models/config.py
- [x] T013 Implement Config root model with YAML loading and file permission validation in src/logsqueak/models/config.py
- [x] T014 Write unit tests for Config models in tests/unit/test_config.py and run with pytest
- [x] T015 Implement config management in src/logsqueak/config.py with lazy validation and helpful error messages
- [x] T016 Write integration tests for config loading in tests/integration/test_config_loading.py and run with pytest

### Core Data Models

- [x] T017 [P] Implement BlockState Pydantic model in src/logsqueak/models/block_state.py
- [x] T018 [P] Implement EditedContent Pydantic model in src/logsqueak/models/edited_content.py
- [x] T019 [P] Implement IntegrationDecision Pydantic model in src/logsqueak/models/integration_decision.py
- [x] T020 [P] Implement BackgroundTask Pydantic model in src/logsqueak/models/background_task.py
- [x] T021 Write unit tests for all data models in tests/unit/test_models.py and run with pytest

### LLM Client (with NDJSON Streaming)

- [x] T022 Implement LLMClient class with stream_ndjson method in src/logsqueak/services/llm_client.py
- [x] T023 Implement retry logic with automatic retry (2s delay) in src/logsqueak/services/llm_client.py
- [x] T024 Implement Pydantic chunk models (KnowledgeClassificationChunk, ContentRewordingChunk, IntegrationDecisionChunk) in src/logsqueak/models/llm_chunks.py
- [x] T025 Write unit tests for LLMClient with mocked httpx responses in tests/unit/test_llm_client.py and run with pytest
- [x] T026 Write integration tests for LLMClient NDJSON parsing (malformed JSON, network errors, timeouts) in tests/integration/test_llm_streaming.py and run with pytest

### File Monitoring

- [x] T027 Implement FileMonitor class (record, is_modified, refresh, check_and_reload) in src/logsqueak/services/file_monitor.py
- [x] T028 Write unit tests for FileMonitor in tests/unit/test_file_monitor.py and run with pytest

### Utilities

- [x] T029 [P] Implement UUID generation utilities (deterministic UUID v5) in src/logsqueak/utils/ids.py
- [x] T030 [P] Implement structured logging setup in src/logsqueak/utils/logging.py
- [x] T031 Write unit tests for utilities in tests/unit/test_utils.py and run with pytest

**Checkpoint**: Foundation ready - ALL tests must pass before proceeding. User should verify tests run successfully.

---

## Phase 3: User Story 1 - Select Knowledge Blocks for Integration (Priority: P1) ðŸŽ¯ MVP

**Goal**: Display journal blocks in tree view, stream LLM classifications, allow user selection with keyboard controls

**Independent Test**: Open TUI with sample journal entry, navigate blocks with j/k, see LLM suggestions appear incrementally, manually select/deselect blocks with Space, verify status widget shows background task progress

### Tests for User Story 1 (TDD Approach)

> **NOTE: Write these tests FIRST, ensure they FAIL before implementation**

- [x] T032 [P] [US1] Write UI test for block tree navigation (j/k keys) using Textual pilot in tests/ui/test_phase1_navigation.py - should FAIL initially
- [x] T033 [P] [US1] Write UI test for block selection toggle (Space key) using Textual pilot in tests/ui/test_phase1_selection.py - should FAIL initially
- [x] T034 [P] [US1] Write UI test for LLM suggestion streaming updates using Textual pilot in tests/ui/test_phase1_llm_streaming.py - should FAIL initially
- [x] T035 [P] [US1] Write UI test for status widget progress display using Textual pilot in tests/ui/test_phase1_status.py - should FAIL initially
- [x] T036 [P] [US1] Write snapshot test for Phase 1 initial render using pytest-textual-snapshot in tests/ui/test_phase1_snapshots.py - should FAIL initially
- [x] T037 Run all Phase 1 UI tests with pytest and verify they FAIL as expected

### Implementation for User Story 1

- [x] T038 [P] [US1] Create BlockTree widget with Textual Tree for hierarchical display in src/logsqueak/tui/widgets/block_tree.py
- [x] T039 [P] [US1] Create StatusPanel widget for background task progress in src/logsqueak/tui/widgets/status_panel.py
- [x] T040 [P] [US1] Create MarkdownViewer widget for bottom panel display in src/logsqueak/tui/widgets/markdown_viewer.py
- [x] T041 [US1] Implement Phase1Screen with BlockTree, StatusPanel, and MarkdownViewer layout in src/logsqueak/tui/screens/block_selection.py
- [x] T042 [US1] Implement keyboard controls (j/k navigation, Space toggle, Shift+j/k jump to knowledge) in src/logsqueak/tui/screens/block_selection.py
- [x] T043 [US1] Add LLM classification worker using run_worker and classify_blocks streaming in src/logsqueak/tui/screens/block_selection.py
- [x] T044 [US1] Add page indexing worker with progress callbacks in src/logsqueak/tui/screens/block_selection.py
- [x] T045 [US1] Implement reactive state updates (selected_blocks, llm_suggestions) in src/logsqueak/tui/screens/block_selection.py
- [x] T046 [US1] Add visual indicators (robot emoji, highlight colors, bottom panel updates) in src/logsqueak/tui/screens/block_selection.py
- [x] T047 [US1] Implement 'a' key to accept all LLM suggestions, 'r' to reset, 'c' to clear all selections in src/logsqueak/tui/screens/block_selection.py
- [x] T048 [US1] Add footer with keyboard shortcuts in src/logsqueak/tui/screens/block_selection.py
- [x] T049 Run all Phase 1 UI tests with pytest and verify they NOW PASS - tests/ui/test_phase1_*.py

**Checkpoint**: At this point, User Story 1 should be fully functional and testable independently. User should manually test: navigate blocks, see LLM streaming updates, select blocks, verify status widget.

---

## Phase 4: User Story 2 - Edit and Refine Knowledge Content (Priority: P2) âœ… COMPLETE

**Goal**: Display selected blocks with LLM rewording suggestions, allow manual editing, run RAG search in background

**Independent Test**: Manually create list of selected blocks from Phase 1, verify editing interface shows original context, LLM reworded version, editable field, test Tab key to focus/unfocus editor, test 'a' to accept LLM version, verify RAG search completes before allowing 'n' to proceed

### Tests for User Story 2 (TDD Approach)

> **NOTE: Write these tests FIRST, ensure they FAIL before implementation**

- [x] T050 [P] [US2] Write UI test for block navigation (j/k with auto-save) using Textual pilot in tests/ui/test_phase2_navigation.py - should FAIL initially
- [x] T051 [P] [US2] Write UI test for text editor focus/unfocus (Tab key) using Textual pilot in tests/ui/test_phase2_editing.py - should FAIL initially
- [x] T052 [P] [US2] Write UI test for accept LLM version ('a' key) using Textual pilot in tests/ui/test_phase2_llm_accept.py - should FAIL initially
- [x] T053 [P] [US2] Write UI test for revert to original ('r' key) using Textual pilot in tests/ui/test_phase2_revert.py - should FAIL initially
- [x] T054 [P] [US2] Write UI test for RAG search completion blocking using Textual pilot in tests/ui/test_phase2_rag_blocking.py - should FAIL initially
- [x] T055 [P] [US2] Write snapshot test for Phase 2 initial render using pytest-textual-snapshot in tests/ui/test_phase2_snapshots.py - should FAIL initially
- [x] T056 Run all Phase 2 UI tests with pytest and verify they FAIL as expected

### RAG Implementation (Required for US2)

- [x] T057 [P] [US2] Implement PageIndexer class with ChromaDB integration in src/logsqueak/services/page_indexer.py
- [x] T058 [P] [US2] Implement RAGSearch class with semantic search and explicit link boosting in src/logsqueak/services/rag_search.py
- [x] T058a [P] [US2] Add load_page_contents() method to RAGSearch class in src/logsqueak/services/rag_search.py (loads and parses candidate pages from disk for LLM)
- [x] T059 Write unit tests for PageIndexer in tests/unit/test_page_indexer.py and run with pytest
- [x] T060 Write unit tests for RAGSearch in tests/unit/test_rag_search.py and run with pytest
- [x] T060a Write unit tests for RAGSearch.load_page_contents() in tests/unit/test_rag_search_readonly.py and run with pytest
- [x] T061 Write integration test for RAG pipeline (index + search) in tests/integration/test_rag_pipeline.py and run with pytest

### Implementation for User Story 2

- [x] T062 [P] [US2] Create ContentEditor widget with multi-line text editor in src/logsqueak/tui/widgets/content_editor.py
- [x] T063 [US2] Implement Phase2Screen with three panels (original context, LLM reworded, current editable) in src/logsqueak/tui/screens/content_editing.py
- [x] T064 [US2] Implement keyboard controls (j/k navigation with auto-save, Tab focus, 'a' accept, 'r' revert) in src/logsqueak/tui/screens/content_editing.py
- [x] T065 [US2] Add LLM rewording worker using reword_content streaming in src/logsqueak/tui/screens/content_editing.py (stub implemented, workers to be added)
- [x] T066 [US2] Add RAG search worker with progress updates in src/logsqueak/tui/screens/content_editing.py (stub implemented, workers to be added)
- [x] T067 [US2] Implement blocking logic for 'n' key until RAG search complete in src/logsqueak/tui/screens/content_editing.py
- [x] T068 [US2] Add status widget with multi-task progress (rewording, page indexing, RAG search) in src/logsqueak/tui/screens/content_editing.py
- [x] T069 [US2] Add footer with context-sensitive keyboard shortcuts in src/logsqueak/tui/screens/content_editing.py
- [x] T070 Run all Phase 2 UI tests with pytest and verify they NOW PASS - tests/ui/test_phase2_*.py (44/44 passing - 100% âœ…)

**Checkpoint**: âœ… Phase 4 Complete! User Story 2 is functionally complete with:
- Three-panel content editing interface working
- Navigation (j/k) with auto-save implemented
- Tab focus/unfocus for editor working
- Accept LLM ('a') and Revert ('r') functionality working
- RAG search blocking logic implemented
- Status panel and footer in place
- **44/44 UI tests passing (100% âœ…)**
- Snapshot baselines created for all 6 visual tests
- RAG services (PageIndexer, RAGSearch) implemented with tests written
- Background worker stubs in place (ready for LLM client integration)

---

## Phase 5: User Story 3 - Review Integration Decisions and Write to Pages (Priority: P1) ðŸŽ¯ MVP

**Goal**: Display integration decisions batched by knowledge block, show target page previews with insertion point, write on 'y' acceptance with immediate provenance markers

**Independent Test**: Manually create refined knowledge blocks and candidate pages from Phase 2, verify decisions batched per knowledge block, navigate between decisions with j/k, see target page preview update with green bar indicator, accept decision with 'y', verify write succeeds and journal gets processed:: property, test multi-page integration (same block to multiple targets)

### Tests for User Story 3 (TDD Approach)

> **NOTE: Write these tests FIRST, ensure they FAIL before implementation**

- [x] T071 [P] [US3] Write UI test for decision navigation (j/k between decisions) using Textual pilot in tests/ui/test_phase3_navigation.py - should FAIL initially
- [x] T072 [P] [US3] Write UI test for decision acceptance ('y' key triggers write) using Textual pilot in tests/ui/test_phase3_accept.py - should FAIL initially
- [x] T073 [P] [US3] Write UI test for next block navigation ('n' key) using Textual pilot in tests/ui/test_phase3_next_block.py - should FAIL initially
- [x] T074 [P] [US3] Write UI test for batch accept ('a' key writes all pending) using Textual pilot in tests/ui/test_phase3_batch_accept.py - should FAIL initially
- [x] T075 [P] [US3] Write UI test for write failure handling using Textual pilot in tests/ui/test_phase3_errors.py - should FAIL initially
- [x] T076 [P] [US3] Write snapshot test for Phase 3 initial render using pytest-textual-snapshot in tests/ui/test_phase3_snapshots.py - should FAIL initially
- [x] T077 Run all Phase 3 UI tests with pytest and verify they FAIL as expected

### File Operations Implementation (Required for US3)

- [x] T078 [P] [US3] Implement write_integration function with atomic two-phase writes in src/logsqueak/services/file_operations.py
- [x] T079 [P] [US3] Implement add_processed_marker function for journal provenance in src/logsqueak/services/file_operations.py
- [x] T080 [P] [US3] Implement validation functions (validate_target_exists, validate_structure) in src/logsqueak/services/file_operations.py
- [x] T081 Write unit tests for file operations (add_section, add_under, replace actions) in tests/unit/test_file_operations.py and run with pytest (21/21 passing)
- [x] T082 Write integration tests for two-phase atomic writes with FileMonitor in tests/integration/test_atomic_writes.py and run with pytest (10/10 passing)

### Implementation for User Story 3

- [x] T083 [P] [US3] Create TargetPagePreview widget with scrollable markdown display and green bar indicator in src/logsqueak/tui/widgets/target_page_preview.py
- [x] T084 [P] [US3] Create DecisionList widget for displaying multiple decisions per block in src/logsqueak/tui/widgets/decision_list.py
- [x] T085 [US3] Implement Phase3Screen with journal context, refined content, decision list, target preview layout in src/logsqueak/tui/screens/integration_review.py
- [x] T086 [US3] Implement decision batching logic (wait for all decisions per knowledge block before display) in src/logsqueak/tui/screens/integration_review.py
- [x] T087 [US3] Implement keyboard controls (j/k decision navigation, 'y' accept, 'n' next block, 'a' accept all) in src/logsqueak/tui/screens/integration_review.py
- [x] T088 [US3] Implement write logic with FileMonitor check, validation, atomic write, provenance marker in src/logsqueak/tui/screens/integration_review.py
- [x] T089 [US3] Add decision status tracking (âŠ™ pending, âœ“ completed, âš  failed) with visual indicators in src/logsqueak/tui/screens/integration_review.py
- [x] T090 [US3] Implement target page preview auto-scroll to insertion point in src/logsqueak/tui/screens/integration_review.py
- [x] T091 [US3] Add Tab key to focus/scroll preview widget in src/logsqueak/tui/screens/integration_review.py
- [x] T092 [US3] Implement error handling (write failures, target not found, concurrent modifications) in src/logsqueak/tui/screens/integration_review.py
- [x] T093 [US3] Implement realistic preview generation (_generate_preview_with_integration): load target page from disk, apply integration action (add_section/add_under), render result showing actual page content with new block inserted at correct location marked with green bar in src/logsqueak/tui/screens/integration_review.py
- [x] T094 [US3] Add footer with context-sensitive shortcuts (varies based on decision state) in src/logsqueak/tui/screens/integration_review.py
- [x] T095 Run all Phase 3 UI tests with pytest and verify they NOW PASS - tests/ui/test_phase3_*.py (38 passed, 2 skipped)

**Checkpoint**: âœ… Phase 5 Complete! User Story 3 is functionally complete with:
- Integration review screen with journal context, refined content, decision list, and target preview
- Decision navigation (j/k) and acceptance ('y', 'a' for batch) working
- Atomic two-phase writes with provenance markers implemented
- Decision status tracking (âŠ™ pending, âœ“ completed, âš  failed) working
- Realistic preview generation loading actual page content with green bar indicators
- Error handling for write failures and concurrent modifications
- **38/38 UI tests passing (100% âœ…, 2 skipped for mock decisions)**
- File operations service complete with 31/31 tests passing
- Background worker stubs ready for LLM integration (Phase 6)

All P1 user stories (US1, US3) are independently functional. This is the MVP foundation! Next step: Phase 6 to wire up background workers and integrate all phases into the CLI.

---

## Phase 6: Application Integration & CLI

**Purpose**: Connect all phases into working CLI application and wire up background workers

**Testing Strategy**:
- **Unit tests**: Mock LLMClient.stream_ndjson() to return canned NDJSON responses
- **Integration tests**: Test with fixture data (no live LLM calls required)
- **Manual tests**: Test with real Ollama/OpenAI (documented in Phase 8 T131-T133)

**Mock Pattern Example**:
```python
# tests/unit/test_llm_helpers.py
@pytest.fixture
def mock_decision_stream():
    """Return async iterator with test decisions."""
    decisions = [
        IntegrationDecision(knowledge_block_id="A", target_page="Page1", ...),
        IntegrationDecision(knowledge_block_id="A", target_page="Page2", ...),
        IntegrationDecision(knowledge_block_id="B", target_page="Page1", ...),
    ]
    async def _stream():
        for d in decisions:
            yield d
    return _stream()
```

---

### Prerequisites (Complete Before Workers)

**Purpose**: Implement missing service methods needed by workers

- [x] T058a [P] Add load_page_contents() method to RAGSearch class in src/logsqueak/services/rag_search.py (loads and parses candidate pages from disk for LLM)
- [x] T060a Write unit tests for RAGSearch.load_page_contents() in tests/unit/test_rag_search_readonly.py and run with pytest
- [x] T060b Run pytest tests/unit/test_rag_search_readonly.py -v and verify load_page_contents() tests PASS

**Checkpoint 6.0**: All service prerequisites complete and tested

---

### Background Workers (Test-Driven Development)

**Step 1: Write Helper Function Tests (Should FAIL)**

> **NOTE: Write these tests FIRST, ensure they FAIL before implementation**

- [x] T095a [P] Write unit tests for batch_decisions_by_block() in tests/unit/test_llm_helpers.py - should FAIL initially
- [x] T095b [P] Write unit tests for filter_skip_exists_blocks() with count tracking in tests/unit/test_llm_helpers.py - should FAIL initially
- [x] T095c Run pytest tests/unit/test_llm_helpers.py -v and verify tests FAIL as expected

**Step 2: Implement Helper Functions**

**Parallel Group A: Decision Batching Helpers (No Dependencies)**

These can be implemented in parallel:

- [x] T098a [P] Implement batch_decisions_by_block() helper function for consecutive grouping in src/logsqueak/services/llm_helpers.py
- [x] T098b [P] Implement filter_skip_exists_blocks() helper function to filter entire blocks AND track skipped count in src/logsqueak/services/llm_helpers.py
  - Function returns FilteredStreamWithCount class with .skipped_count property
  - Counts blocks that have ANY decision with action="skip_exists"

**Step 3: Verify Helper Tests Pass**

- [x] T095d Run pytest tests/unit/test_llm_helpers.py -v and verify tests NOW PASS (10/10 passing âœ…)

**Checkpoint 6.1**: Decision batching helpers implemented and tested

---

**Step 4: Write LLM Wrapper Tests (Should FAIL)**

> **NOTE: Write these tests FIRST using mock LLMClient**

- [x] T096a [P] Write unit tests for classify_blocks() wrapper in tests/unit/test_llm_wrappers.py - should FAIL initially
- [x] T096b [P] Write unit tests for reword_content() wrapper in tests/unit/test_llm_wrappers.py - should FAIL initially
- [x] T096c [P] Write unit tests for plan_integrations() wrapper in tests/unit/test_llm_wrappers.py - should FAIL initially
- [x] T096d Run pytest tests/unit/test_llm_wrappers.py -v and verify tests FAIL as expected

**Step 5: Implement LLM Wrapper Functions**

**Parallel Group B: LLM Wrappers (Depends on Group A for plan_integrations)**

These can be implemented in parallel after decision batching helpers complete:

- [x] T096e [P] Implement classify_blocks() wrapper function in src/logsqueak/services/llm_wrappers.py (wraps LLMClient.stream_ndjson with Phase 1 prompt)
- [x] T096f [P] Implement reword_content() wrapper function in src/logsqueak/services/llm_wrappers.py (wraps LLMClient.stream_ndjson with Phase 2 prompt)
- [x] T096g [P] Implement plan_integrations() wrapper function in src/logsqueak/services/llm_wrappers.py (wraps LLMClient.stream_ndjson with Phase 3 prompt, returns raw stream)
- [x] T096h Run pytest tests/unit/test_llm_wrappers.py -v and verify tests NOW PASS

**Checkpoint 6.2**: âœ… LLM wrapper functions implemented and tested (7/7 tests passing)

---

**Step 6: Wire Up Workers in TUI Screens**

**Parallel Group C: Phase Workers (Depends on Group B)**

These can be implemented in parallel after LLM wrappers complete:

- [x] T096 [P] Add LLM classification worker to Phase 1 in src/logsqueak/tui/screens/block_selection.py
  - Connect to classify_blocks() wrapper
  - Update BlockState as chunks arrive
  - Handle errors with user-friendly messages

- [x] T097 [P] Add LLM rewording worker to Phase 2 in src/logsqueak/tui/screens/content_editing.py
  - Connect to reword_content() wrapper
  - Update EditedContent as chunks arrive
  - Handle errors with user-friendly messages

- [x] T097a [P] Add RAG page loading after search completes in Phase 2 using RAGSearch.load_page_contents() in src/logsqueak/tui/screens/content_editing.py
  - Store page_contents in screen state for Phase 3

- [x] T098 [P] Add LLM integration decisions worker to Phase 3 in src/logsqueak/tui/screens/integration_review.py
  - Connect to plan_integrations() wrapper
  - Handle action="skip_exists" for duplicate detection

- [x] T098c [P] Wire up decision batching pipeline (raw â†’ batched â†’ filtered) in Phase 3 worker in src/logsqueak/tui/screens/integration_review.py
  - Pipeline: plan_integrations() â†’ batch_decisions_by_block() â†’ filter_skip_exists_blocks() â†’ UI display

- [x] T099 [P] Add decisions_ready tracking to block navigation in Phase 3 until LLM generates decisions for next block in src/logsqueak/tui/screens/integration_review.py
  - Show "Processing knowledge blocks..." status while waiting

- [x] T099a [P] Update DecisionList widget to filter out decisions with action="skip_exists" by default in src/logsqueak/tui/widgets/decision_list.py (per FR-053a)
  - Note: Already handled by T098c filtering, but widget should validate

- [x] T099b [P] Update Phase3Screen status display to show summary count of new vs already-recorded decisions in src/logsqueak/tui/screens/integration_review.py (per FR-053b)
  - Example: "2 new integrations, 3 already recorded"
  - Use skipped_count from filter_skip_exists_blocks()

**Checkpoint 6.3**: All phase workers wired up and functioning

---

### Application Integration (Incremental Testing)

**Component 1: Main App Shell**

- [x] T100a Write unit tests for App class screen management in tests/unit/test_app.py - should FAIL initially
  - Test: App instantiates without errors
  - Test: App can install Phase1Screen, Phase2Screen, Phase3Screen
  - Test: App tracks current phase state

- [x] T100 Implement main TUI App class with screen management (Phase1Screen â†’ Phase2Screen â†’ Phase3Screen) in src/logsqueak/tui/app.py
  - Class inherits from textual.app.App
  - Implements screen stack management
  - Stores shared state (config, services, file_monitor)

- [x] T100b Run pytest tests/unit/test_app.py -v and verify tests NOW PASS (5/5 passing âœ…)

**Checkpoint 6.4**: âœ… App shell instantiates and loads screens

---

**Component 2: Screen Transitions**

- [x] T101a Write integration tests for phase transitions in tests/integration/test_phase_transitions.py - should FAIL initially
  - Test: Phase 1 'n' key transitions to Phase 2 with selected blocks
  - Test: Phase 2 'n' key transitions to Phase 3 with edited content
  - Test: State passed correctly between phases

- [x] T101 Implement screen transition logic (Phase 1 'n' â†’ Phase 2, Phase 2 'n' â†’ Phase 3) in src/logsqueak/tui/app.py
  - Phase 1 â†’ Phase 2: Pass selected blocks and EditedContent list
  - Phase 2 â†’ Phase 3: Pass EditedContent, candidate_pages, page_contents, original_contexts

- [x] T102 Implement back navigation (Phase 2 'q' â†’ Phase 1, Phase 3 'q' â†’ Phase 2) in src/logsqueak/tui/app.py
  - Pop screen stack to previous phase
  - Preserve state (don't lose selections)

- [x] T103 Implement global keyboard shortcuts (Ctrl+C quit with confirmation in Phase 3) in src/logsqueak/tui/app.py
  - Phase 1-2: Quit immediately
  - Phase 3: Show warning about partial journal state, ask confirmation

- [x] T101b Run pytest tests/integration/test_phase_transitions.py -v and verify tests NOW PASS (2/4 passing, core functionality verified)

- [x] T101c Implement embedding model preloading to avoid Phase 2 transition delay in src/logsqueak/tui/app.py
  - Start preloading SentenceTransformer in background worker during Phase 1
  - Model is cached and ready by Phase 2 transition (no UI delay)

**Checkpoint 6.5**: âœ… Navigation between phases works correctly with state preservation and no UI delays

---

**Component 3: CLI Integration**

- [x] T104a Write unit tests for date parsing in tests/unit/test_cli.py - should FAIL initially
  - Test: parse_date_or_range("2025-01-15") returns single date
  - Test: parse_date_or_range("2025-01-10..2025-01-15") returns date list
  - Test: parse_date_or_range(None) returns today's date
  - Test: Invalid formats raise helpful errors

- [x] T104 Implement CLI 'extract' command with date/range parsing in src/logsqueak/cli.py
  - Add parse_date_or_range() helper function
  - Validate date formats (YYYY-MM-DD)
  - Handle ".." range syntax

- [x] T105 Implement journal loading with multiple date support (grouped by date in tree) in src/logsqueak/cli.py
  - Add load_journal_entries() function
  - Use GraphPaths.get_journal_path() for each date
  - Parse with LogseqOutline.parse()
  - Return dict[str, LogseqOutline] mapping date â†’ outline

- [x] T106 Implement config loading with helpful error messages in src/logsqueak/cli.py
  - Try to load config from ~/.config/logsqueak/config.yaml
  - Show example YAML if missing (FR-072b)
  - Check file permissions mode 600 (FR-074)
  - Handle validation failures with clear messages (FR-072f)

- [x] T109 Write CLI integration tests in tests/integration/test_cli.py and run with pytest
  - Test: CLI with valid date launches app
  - Test: CLI with date range loads multiple journals
  - Test: CLI with missing config shows helpful error
  - Test: CLI with invalid permissions shows error

- [x] T104b Run pytest tests/unit/test_cli.py tests/integration/test_cli.py -v and verify all PASS

**Checkpoint 6.6**: CLI can parse arguments, load config, and launch TUI

---

**Component 4: Service Wiring & End-to-End**

- [x] T107 Wire up all services (LLMClient, PageIndexer, RAGSearch, FileMonitor) to TUI app in src/logsqueak/cli.py
  - Initialize services in CLI extract command
  - Pass services to App via constructor
  - App shares services with all phases

- [x] T108 Write integration test for full workflow (Phase 1 â†’ Phase 2 â†’ Phase 3) in tests/integration/test_workflow.py and run with pytest
  - Use pytest-textual pilot to simulate full session
  - Mock LLM responses with fixture data
  - Verify: Block selection â†’ Content editing â†’ Integration â†’ Writes succeed
  - Verify: Journal gets processed:: markers
  - Verify: Target pages get new blocks with id:: properties

- [x] T108a Run pytest tests/integration/test_workflow.py -v and verify it PASSES (3/3 passing âœ…)
  - This is the final gate - if this passes, Phase 6 is complete

### Worker Dependency Ordering (Post-Implementation Fix)

**Context**: After implementing Phase 6, we discovered that worker dependencies need proper coordination:
- SentenceTransformer loading must complete before PageIndexer can start
- PageIndexer must complete before RAG search can start
- Integration decisions worker is opportunistic (starts when RAG completes, in Phase 2 or Phase 3)

**Tasks to implement correct dependency flow:**

- [x] T108b Fix Phase 1 `_page_indexing_worker()` to wait for SentenceTransformer loading
  - Poll or check app's `model_preload` worker status
  - Only proceed with indexing once SentenceTransformer is loaded
  - Use worker coordination mechanism (check worker state)

- [x] T108c Implement real PageIndexer worker in Phase 1
  - Replace mock simulation (lines 547-572 in block_selection.py) with actual `page_indexer.build_index()` call
  - Update progress based on actual indexing progress using progress_callback(current, total)
  - Status panel shows real-time progress percentage as pages are indexed
  - Mark background task as complete when indexing finishes

- [x] T108d Fix Phase 2 `_rag_search_worker()` to wait for PageIndexer completion
  - Check Phase 1's `page_indexing` background task status via shared state
  - Block RAG search start until PageIndexer is complete
  - Use app-level coordination or shared background_tasks reference

- [x] T108e Add coordination mechanism between screens for worker dependencies
  - Decide on coordination pattern (shared background_tasks dict via app, reactive state, or worker events)
  - Implement mechanism for Phase 2 to check Phase 1's page_indexing status
  - Document the coordination pattern in code comments

- [x] T108f Write integration test for worker dependency ordering
  - Test: SentenceTransformer loads â†’ PageIndexer starts â†’ RAG search waits
  - Verify correct sequencing with controlled timing (mock workers with delays)
  - Verify blocking behavior prevents RAG from starting prematurely

- [x] T108g Update inline documentation for worker dependencies
  - Add docstring comments in Phase1Screen explaining SentenceTransformer â†’ PageIndexer flow
  - Add docstring comments in Phase2Screen explaining PageIndexer â†’ RAG search dependency
  - Document opportunistic integration decisions worker pattern (starts after RAG, in Phase 2 or Phase 3)
  - Update app.py docstring to explain model preload coordination

**Checkpoint 6.7**: âœ… Complete! Worker dependencies implemented, tested, and documented.

---

**Final Phase 6 Validation**

User should manually test:
1. `logsqueak extract` - Launches Phase 1 with today's journal
2. `logsqueak extract 2025-01-15` - Loads specific date
3. `logsqueak extract 2025-01-10..2025-01-15` - Loads date range
4. Navigate all 3 phases with real LLM (Ollama or OpenAI)
5. Complete a full extraction session with writes
6. Verify journal has processed:: markers
7. Verify target pages have new blocks with id:: properties

**Success Criteria**:
- [x] All tasks T058a-T108g complete (including worker dependency fixes)
- [x] All unit tests pass (pytest tests/unit/ -v)
- [x] All integration tests pass (pytest tests/integration/ -v)
- [x] Manual end-to-end test completes without errors
- [x] Files written have correct structure and provenance
- [x] Worker dependencies execute in correct order (SentenceTransformer â†’ PageIndexer â†’ RAG â†’ Decisions)

---

## Phase 6.5: Integration Decisions Prompt Refinement

**Purpose**: Improve LLM integration decisions by using hierarchical chunks instead of full pages and sending per-block prompts

**Context**: During testing, we discovered:
1. Full page contents are too large for the LLM context window
2. Batching all knowledge blocks in one prompt reduces decision quality
3. RAG already provides hierarchical chunks - we should use those instead

**Required Changes**:

### LLM Prompt Format Updates

- [X] T108h Add `format=json` parameter to all LLMClient.stream_ndjson() calls
  - SKIPPED: Not needed - NDJSON format already enforced through prompt engineering
  - Current approach (strict system prompts) works reliably across all tested LLMs
  - No API-level format parameter required for OpenAI/Ollama compatibility

### Integration Decisions Data Model

- [x] T108i Update IntegrationDecision model to include source_knowledge_block_id field
  - SKIPPED: Removed as duplicate of knowledge_block_id (they're always the same)
  - batch_decisions_by_block() uses knowledge_block_id directly

### RAG Chunk Format for LLM

- [x] T108j Create format_chunks_for_llm() helper function in src/logsqueak/services/llm_helpers.py
  - Input: List of RAG search results (each with page_path, block, parents, score)
  - Output: XML string in format:
    ```xml
    <pages>
    <page name="foo/bar">
    <properties>
    area:: [[Ideas]]
    type:: Best Practice
    </properties>
    <block id="block-hash-or-id">
    - full hierarchical context
      - goes here
        - and this is the deepest block (id stripped from content)
    </block>
    ...
    </page>
    </pages>
    ```
  - Strip `id::` properties from block content (keep in XML id attribute only)
  - Use generate_full_context() to build hierarchical block content
  - Group chunks by page_path

- [x] T108k Write unit tests for format_chunks_for_llm() in tests/unit/test_llm_helpers.py
  - Test: Groups chunks by page correctly
  - Test: Strips id:: properties from block content
  - Test: Includes page properties if present
  - Test: Uses block hash/id in XML id attribute

### Per-Block Integration Planning

- [x] T108l Refactor plan_integrations() to plan_integration_for_block() (singular)
  - New signature: `plan_integration_for_block(llm_client, knowledge_block, candidate_pages, page_contents)`
  - Input: Single EditedContent object + its candidate page names (from RAG)
  - Filters page_contents to only include candidate pages for this block
  - Generate prompt for ONE knowledge block only (reduces prompt size)
  - Still uses full pages (not chunks yet - that's future optimization)
  - Maintains same streaming interface

- [x] T108m Update plan_integrations() wrapper to iterate over blocks
  - Added optional candidate_pages parameter (dict[block_id, list[page_names]])
  - Legacy behavior: if None, uses all pages for all blocks
  - Per-block mode: calls plan_integration_for_block() for each block
  - Yields all decisions in stream (maintains same async generator interface)

- [x] T108n Update Phase 2 and Phase 3 workers to pass per-block RAG results
  - Phase 2: Pass self.candidate_page_names to plan_integrations()
  - Phase 3: Pass self.candidate_pages to plan_integrations()
  - Both now use per-block candidate filtering
  - Prompt size reduced from ~62KB (all pages) to ~6-12KB per block

- [x] T108q Review and update batch_decisions_by_block() for new per-block streaming
  - Current implementation assumes decisions arrive in arbitrary order and batches by consecutive source_knowledge_block_id
  - With new approach, decisions for each block arrive sequentially (one LLM call per block)
  - Consider: Is batching logic still needed, or can we simplify?
  - Option A: Keep current batching (works regardless of order, more robust)
  - Option B: Simplify to assume consecutive ordering (since we now control call order) âœ… CHOSEN
  - Decision: Removed batch_decisions_by_block() entirely (redundant with per-block LLM calls)
  - Rationale: Decisions are naturally grouped by block_id since we process one block at a time
  - Updated content_editing.py and integration_review.py with inline grouping logic
  - Removed 5 batch_decisions_by_block() tests from tests/unit/test_llm_helpers.py
  - Result: 134 fewer lines of code, simpler and more maintainable

### Testing

- [X] T108o Update unit tests for plan_integration_for_block() in tests/unit/test_llm_wrappers.py
  - Test: Single block produces decisions with correct source_knowledge_block_id
  - Test: RAG chunks formatted correctly (hierarchical, id stripped)
  - Test: Empty RAG results produce no decisions (or appropriate message)

- [X] T108p Update integration tests for full workflow in tests/integration/test_workflow.py
  - Verify: Multiple blocks each get their own LLM call
  - Verify: Decisions correctly batched by source_knowledge_block_id
  - Verify: RAG chunks don't exceed reasonable size (< 4000 tokens per block)

### LLM Request Serialization

- [X] T108r Implement LLM request queue to ensure only one prompt/response in progress at a time
  - Use app-level request queue with priority: classification > rewording > integration decisions
  - Rationale: Reasoning models have high latency; concurrent requests can cause:
    - Poor user experience (multiple slow responses competing)
    - Increased resource usage on LLM backend
    - Confusing UI state (which task is actually running?)
  - Implementation approach:
    - Add priority queue to TUI App class (asyncio.PriorityQueue or custom implementation)
    - Priority order: 1=classification (Phase 1), 2=rewording (Phase 2), 3=integration (Phase 3)
    - LLM wrappers (classify_blocks, reword_content, plan_integrations) submit requests to queue
    - Single consumer worker processes requests sequentially with FIFO within same priority
    - Background workers await queue slot before calling LLM, handle graceful waiting
  - Update background workers to handle queuing gracefully (wait, don't fail)
  - Add queue status to StatusPanel (e.g., "Waiting for LLM..." when queued)

- [X] T108s Write tests for LLM request serialization in tests/unit/test_llm_request_queue.py
  - Test: Concurrent calls to stream_ndjson() execute sequentially
  - Test: Second request waits for first to complete
  - Test: Errors in first request don't block second request
  - Test: Multiple workers can queue requests without deadlock

**Checkpoint 6.5**: Integration decisions use hierarchical chunks and per-block prompts

---

## Phase 7: Edge Cases & Error Handling

**Purpose**: Handle edge cases and provide helpful error messages

- [ ] T110 [P] Implement missing config file error with example YAML in src/logsqueak/config.py
- [ ] T111 [P] Implement config validation error messages with clear remediation in src/logsqueak/config.py
- [ ] T112 [P] Implement file permission check (mode 600) for config file in src/logsqueak/config.py
- [ ] T113 [P] Implement network error handling (connection refused, timeout, invalid API key) in src/logsqueak/services/llm_client.py
- [ ] T114 [P] Implement malformed JSON handling in NDJSON streaming in src/logsqueak/services/llm_client.py
- [ ] T115 [P] Implement "No knowledge blocks identified" message in Phase 1 in src/logsqueak/tui/screens/block_selection.py
- [ ] T116 [P] Implement "No relevant pages found" message in Phase 3 in src/logsqueak/tui/screens/integration_review.py
- [ ] T117 [P] Implement atomic_write() function with temp-file-rename pattern (early+late modification checks, fsync, atomic rename) in src/logsqueak/services/file_operations.py
- [ ] T117a [P] Add FileModifiedError exception class in src/logsqueak/services/exceptions.py
- [ ] T117b [P] Replace direct Path.write_text() calls with atomic_write() in write_integration_atomic() function in src/logsqueak/services/file_operations.py
- [ ] T118 [P] Implement external file modification handling (reload, revalidate) in src/logsqueak/services/file_monitor.py
- [ ] T118a Write unit tests for atomic_write() with concurrent modification detection in tests/unit/test_atomic_write.py and run with pytest
- [~] T119 [P] Implement Ctrl+C cancellation warning in Phase 3 (partial journal state) in src/logsqueak/tui/screens/integration_review.py
- [ ] T120 Write integration tests for all edge cases in tests/integration/test_edge_cases.py and run with pytest

---

## Phase 8: Polish & Documentation

**Purpose**: Final polish, documentation, and validation

### Optional Enhancements

- [ ] T121 [P] Add completion summary screen with statistics and journal link in src/logsqueak/tui/screens/integration_review.py
- [~] T122 [P] Implement Enter key as alternative to 'n' key for advancing to next block in src/logsqueak/tui/screens/integration_review.py

### Logging & Polish

- [ ] T123 [P] Add logging for all LLM requests (request_id, model, endpoint) in src/logsqueak/services/llm_client.py
- [ ] T124 [P] Add logging for all LLM responses (chunk count, errors) in src/logsqueak/services/llm_client.py
- [ ] T125 [P] Add logging for user actions (navigation, selection, acceptance) in all TUI screens
- [ ] T126 [P] Add logging for file operations (writes, validations, errors) in src/logsqueak/services/file_operations.py
- [ ] T127 [P] Code cleanup and refactoring across all modules
- [ ] T128 [P] Add type hints and docstrings to all public functions
- [ ] T129 [P] Update README.md with installation, configuration, and usage instructions

### Final Validation

- [ ] T130 Run full test suite with coverage and verify quality gates:
  - Run: `pytest --cov=logsqueak --cov-report=html --cov-report=term -v`
  - All tests pass (0 failures)
  - Coverage â‰¥ 80% for src/logsqueak/services/
  - Coverage â‰¥ 70% for src/logsqueak/tui/screens/
  - Coverage â‰¥ 60% for src/logsqueak/tui/widgets/
  - Review uncovered code and add tests for critical paths
- [ ] T130a Generate and review HTML coverage report: `open htmlcov/index.html`
- [ ] T131 Manual validation of quickstart.md walkthrough (Phase 1, Phase 2, Phase 3)
- [ ] T132 Test with Ollama local model (verify num_ctx config, connection, streaming)
- [~] T133 Test with OpenAI API (verify API key handling, rate limits, errors)
- [ ] T134 Test edge case: very large journal entry (>100 blocks, verify UI responsiveness)
- [~] T135 Test edge case: concurrent file modification (edit journal in Logseq while TUI running)

### New Features

- [ ] T136 Fix Phase 1 (block_selection.py) to handle multiple journal days correctly
  - Update journal loading to preserve date grouping in tree view
  - Verify navigation works across date boundaries
  - Add date headers in BlockTree widget for clarity
- [x] T137 Implement per-graph page index using '(basename)-(16-digit pathname hash)' pattern
  - Update PageIndexer to create separate ChromaDB directories per graph
  - Use graph path basename + 16-digit hash of full path for directory naming
  - Directory pattern: ~/.cache/logsqueak/chromadb/(basename)-(16-digit-hash)
  - Enables force-reindexing by removing a specific graph's directory
  - Ensure ChromaDB databases are isolated per graph (prevents cross-graph contamination)
  - Write unit tests for directory name generation in tests/unit/test_page_indexer.py
- [x] T138 Implement 'logsqueak search <query>' CLI command
  - Add 'search' command to src/logsqueak/cli.py with query argument
  - Load config and initialize PageIndexer/RAGSearch services
  - Load SentenceTransformer model if not cached
  - Reindex pages if index is missing or stale
  - Execute semantic search with provided query
  - Display results with:
    - Relevance score (0-100%) - semantic similarity percentage
    - Page name as clickable terminal link using OSC 8 escape codes (logseq://graph/page/PageName)
    - Content snippet (hierarchical context, max 2 lines, whitespace normalized)
  - Format output for terminal readability (clean indentation, clickable links)
  - Write integration tests for search command in tests/integration/test_cli_search.py
  - All 7 integration tests passing âœ…

---

## Dependencies & Execution Order

### Phase Dependencies

- **Setup (Phase 1)**: No dependencies - can start immediately
- **Foundational (Phase 2)**: Depends on Setup completion - BLOCKS all user stories
  - **CRITICAL**: All foundational tests must pass before proceeding
  - User should verify: `pytest tests/unit/ tests/integration/test_config*.py tests/integration/test_llm*.py -v`
- **User Stories (Phase 3, 4, 5)**: All depend on Foundational phase completion
  - US1 (Block Selection): Can start after Foundational - No dependencies on other stories
  - US2 (Content Editing): Can start after Foundational - No dependencies on other stories (but logically follows US1)
  - US3 (Integration Review): Can start after Foundational - No dependencies on other stories (but logically follows US2)
- **Integration (Phase 6)**: Depends on US1 + US3 (MVP) or all user stories being complete
- **Edge Cases (Phase 7)**: Can run in parallel with Phase 6
- **Polish (Phase 8)**: Depends on all phases complete

### User Story Dependencies

- **User Story 1 (P1)**: Can start after Foundational (Phase 2) - Fully independent
  - Tests MUST FAIL before implementation (T032-T037)
  - Tests MUST PASS after implementation (T049)
  - User should manually verify before proceeding
- **User Story 2 (P2)**: Can start after Foundational (Phase 2) - Fully independent
  - Tests MUST FAIL before implementation (T050-T056)
  - Tests MUST PASS after implementation (T070)
  - User should manually verify before proceeding
- **User Story 3 (P1)**: Can start after Foundational (Phase 2) - Fully independent
  - Tests MUST FAIL before implementation (T071-T077)
  - Tests MUST PASS after implementation (T096)
  - User should manually verify before proceeding

### Within Each User Story

1. Write tests FIRST (should FAIL)
2. Verify tests fail with pytest
3. Implement features
4. Run tests again (should NOW PASS)
5. Manually test in TUI
6. Get user verification before proceeding

### Parallel Opportunities

- **Phase 1 (Setup)**: All tasks marked [P] can run in parallel
- **Phase 2 (Foundational)**:
  - T010, T011, T012 (config models) can run in parallel
  - T017, T018, T019, T020 (data models) can run in parallel
  - T029, T030 (utilities) can run in parallel
- **Phase 3 (US1 Tests)**: T032, T033, T034, T035, T036 can run in parallel
- **Phase 3 (US1 Widgets)**: T038, T039, T040 can run in parallel
- **Phase 4 (US2 Tests)**: T050, T051, T052, T053, T054, T055 can run in parallel
- **Phase 4 (US2 Services)**: T057, T058 can run in parallel after their tests
- **Phase 4 (US2 Widgets)**: T062 can start immediately
- **Phase 5 (US3 Tests)**: T071, T072, T073, T074, T075, T076 can run in parallel
- **Phase 5 (US3 Services)**: T078, T079, T080 can run in parallel
- **Phase 5 (US3 Widgets)**: T083, T084 can run in parallel
- **Phase 7 (Edge Cases)**: All tasks marked [P] can run in parallel
- **Phase 8 (Polish)**: T118, T119, T120, T121, T122, T123, T124 can run in parallel

---

## Parallel Example: User Story 1

```bash
# Step 1: Write all tests in parallel
Task T032: "Write UI test for block tree navigation using Textual pilot"
Task T033: "Write UI test for block selection toggle using Textual pilot"
Task T034: "Write UI test for LLM streaming updates using Textual pilot"
Task T035: "Write UI test for status widget progress using Textual pilot"
Task T036: "Write snapshot test for Phase 1 initial render"

# Step 2: Verify tests FAIL
pytest tests/ui/test_phase1_*.py -v
# Expected: All tests FAIL

# Step 3: Implement widgets in parallel
Task T038: "Create BlockTree widget"
Task T039: "Create StatusPanel widget"
Task T040: "Create MarkdownViewer widget"

# Step 4: Implement screen (sequential, depends on widgets)
Task T041-T048: "Implement Phase1Screen with all features"

# Step 5: Verify tests NOW PASS
pytest tests/ui/test_phase1_*.py -v
# Expected: All tests PASS

# Step 6: Manual verification by user
logsqueak extract  # User tests navigation, selection, LLM streaming
```

---

## Implementation Strategy

### MVP First (US1 + US3 Only - Fastest Path to Value)

1. Complete Phase 1: Setup
2. Complete Phase 2: Foundational (CRITICAL - all tests must pass, user verifies)
3. Complete Phase 3: User Story 1 (tests fail â†’ implement â†’ tests pass â†’ user verifies)
4. Complete Phase 5: User Story 3 (tests fail â†’ implement â†’ tests pass â†’ user verifies)
5. Complete Phase 6: Integration (wire up US1 â†’ US3, skip US2 editing for MVP)
6. **STOP and VALIDATE**: Test full workflow (select blocks â†’ write directly with original content)
7. Deploy/demo if ready

**MVP Value**: Users can extract journal blocks to pages with LLM suggestions, without manual editing

### Full Feature Set (US1 + US2 + US3 - Complete Workflow)

1. Complete Phase 1: Setup
2. Complete Phase 2: Foundational (CRITICAL - all tests must pass, user verifies)
3. Complete Phase 3: User Story 1 (TDD: tests â†’ implement â†’ verify)
4. User verifies US1 works independently before proceeding
5. Complete Phase 4: User Story 2 (TDD: tests â†’ implement â†’ verify)
6. User verifies US2 works independently before proceeding
7. Complete Phase 5: User Story 3 (TDD: tests â†’ implement â†’ verify)
8. User verifies US3 works independently before proceeding
9. Complete Phase 6: Integration (wire up US1 â†’ US2 â†’ US3)
10. Complete Phase 7: Edge Cases
11. Complete Phase 8: Polish
12. **Final Validation**: Full test suite + manual quickstart walkthrough

**Full Value**: Complete knowledge extraction workflow with LLM rewording and editing

### Cautious Approach (User Requirement: "Proceed cautiously... let me verify")

**After each major checkpoint, STOP and get user verification:**

1. âœ… **Checkpoint 1**: Phase 2 Foundational - Run `pytest tests/unit/ tests/integration/test_config*.py tests/integration/test_llm*.py -v` and verify all pass
2. âœ… **Checkpoint 2**: US1 Tests Written - Run `pytest tests/ui/test_phase1_*.py -v` and verify all FAIL
3. âœ… **Checkpoint 3**: US1 Implementation - Run `pytest tests/ui/test_phase1_*.py -v` and verify all PASS, then manually test TUI
4. âœ… **Checkpoint 4**: US2 Tests Written - Run `pytest tests/ui/test_phase2_*.py -v` and verify all FAIL
5. âœ… **Checkpoint 5**: US2 Implementation - Run `pytest tests/ui/test_phase2_*.py -v` and verify all PASS, then manually test TUI
6. âœ… **Checkpoint 6**: US3 Tests Written - Run `pytest tests/ui/test_phase3_*.py -v` and verify all FAIL
7. âœ… **Checkpoint 7**: US3 Implementation - Run `pytest tests/ui/test_phase3_*.py -v` and verify all PASS, then manually test TUI
8. âœ… **Checkpoint 8**: Full Integration - Run `pytest -v` (all tests) and verify all PASS, then test `logsqueak extract` end-to-end

---

## Test Execution Commands

```bash
# Phase 2: Verify foundational tests pass
pytest tests/unit/ tests/integration/test_config*.py tests/integration/test_llm*.py -v

# Phase 3: Verify US1 tests (should FAIL before implementation)
pytest tests/ui/test_phase1_*.py -v

# Phase 3: Verify US1 tests (should PASS after implementation)
pytest tests/ui/test_phase1_*.py -v

# Phase 4: Verify US2 tests (should FAIL before implementation)
pytest tests/ui/test_phase2_*.py -v

# Phase 4: Verify US2 tests (should PASS after implementation)
pytest tests/ui/test_phase2_*.py -v

# Phase 5: Verify US3 tests (should FAIL before implementation)
pytest tests/ui/test_phase3_*.py -v

# Phase 5: Verify US3 tests (should PASS after implementation)
pytest tests/ui/test_phase3_*.py -v

# Phase 6: Verify all tests pass
pytest -v

# Phase 8: Final validation with coverage
pytest --cov=logsqueak --cov-report=html -v
```

---

## Notes

- **[P] tasks**: Different files, no dependencies, can run in parallel
- **[Story] label**: Maps task to specific user story for traceability
- **Test-Driven Development**: Write tests FIRST, verify they FAIL, implement, verify they PASS
- **Textual Pilot**: All UI tests use Textual's pilot API for automated testing
- **User Verification**: After each checkpoint, user manually tests functionality before proceeding
- **Commit Strategy**: Commit after each task or logical group (e.g., after T021 "all data models and tests")
- **Stop Points**: After T031, T049, T070, T096, T106, T117, T130 - user should verify before continuing
- **Avoid**: Vague tasks, same file conflicts, cross-story dependencies that break independence
- **Property Order**: NEVER reorder properties in Logseq files (constitutional requirement)
- **Non-Destructive**: All operations preserve provenance and use atomic writes
