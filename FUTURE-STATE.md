## **Logseq Semantic Merge Pipeline: Implementation Plan**

This document details the complete, 5-phase pipeline for intelligently integrating Logseq journal entries into permanent pages. The system is designed to be efficient, using a hybrid-ID system and a multi-stage LLM approach to balance local performance with commercial-grade text generation.

### **Core Stack**

* **Language:** A general-purpose scripting language (e.g., Python 3.10+).  
* **Core Libraries:**  
  * A library to interface with local LLMs (e.g., Ollama) or commercial generative LLMs (e.g., OpenAI).  
  * A library for a local, persistent vector database (e.g., ChromaDB).  
  * Standard libraries for file/path management, JSON, hashing, and regular expressions.  
* **Core Requirement:** A **round-trip-safe Markdown parser/writer** module. This module must be capable of reading a Logseq-flavored Markdown file into an Abstract Syntax Tree (AST) and serializing that AST back to text without data or formatting loss.

### **Phase 0: Incremental & Cached Graph Indexing**

**Goal:** To create and maintain a persistent vector store that is an up-to-date mirror of your pages/ directory, with a robust ID system for every block.

**Models:**

* **Embedding:** A local embedding model.

**Key Components:**

1. **Vector Store:** A persistent, local vector database instance.  
2. **Cache Manifest:** A text file (e.g., in JSON format) that stores a key-value map of page\_name to its last-known file\_modification\_timestamp.

#### **Helper Logic: Page Parsing and Chunking**

This is the core parsing logic, implementing **Full Context Chunking** and **Hybrid-ID Generation**.

1. **Parse to Tree:** Use the round-trip-safe parser module to read a page file into an AST.  
2. **Recursive Traversal:** Implement a recursive function to walk the AST and generate a flat list of all chunks.  
3. **Inside the Traversal Logic:**  
   * For each node in the AST, determine its content, indent\_level, and children.  
   * Generate the "Full Context Chunk" text by prepending its parent's "Full Context Chunk" text.  
   * Check the node's content for a persistent id:: property using a regular expression.  
   * **If an id:: is found:** The target\_id for this chunk is that persistent ID string.  
   * **If an id:: is NOT found:** The target\_id is a stable content hash (e.g., MD5) of the "Full Context Chunk" text.  
   * The final chunk object will contain the chunk's full text and its metadata (the page\_name and the generated target\_id).  
4. This logic returns a flat list of all chunk objects for the page.

#### **Main Indexing Logic**

1. **Init:** Load the Cache Manifest file and initialize the vector database collection.  
2. **Scan Disk:** Get a list of all current pages in the pages/ directory and their current file modification timestamps.  
3. **Find Deletions:** Loop through the manifest. If a page\_name from the manifest is not on disk:  
   * Delete all vectors from the database where the metadata matches that page\_name.  
   * Remove the entry from the manifest data.  
4. **Find Updates & Additions:** Loop through the pages currently on disk.  
   * If a page is not in the manifest, or if its modification timestamp is newer than the one in the manifest:  
     * Log that the page is being indexed.  
     * **Delete Old:** Delete all existing vectors from the database where the metadata matches that page\_name.  
     * **Parse & Chunk:** Run the helper logic above to get all chunk objects for the page.  
     * **Add New:** Add the new chunks' text, embeddings (generated by the local embedding model), and metadata to the vector database, using the target\_id as the database ID.  
     * **Update Manifest:** Update the manifest data with the new file modification timestamp.  
5. **Save:** Save the updated manifest data back to the JSON file.

### **Phase 1: Knowledge Extraction (Local LLM)**

**Goal:** Get context-rich "Knowledge Packages" from a single, user-specified journal file.

**Model:**

* **LLM:** A fast, local, instruction-following Language Model.

#### **Logic**

1. **Parse (Code):** Read the journal file and use the parser module to parse it into an AST (journal\_tree).  
2. **Identify (LLM):** Send the *full file content* (as raw text) to the local LLM.  
   * **Prompt:** The prompt should instruct the LLM to act as a knowledge classifier. It must identify *only* the specific blocks containing "evergreen knowledge" (insights, facts, etc.) and ignore "activity logging." It must return a structured list (e.g., JSON) of the *exact text* of these blocks.  
3. **Contextualize (Code):**  
   * Parse the LLM's structured response.  
   * Create an empty list for "Knowledge Packages."  
   * Iterate through the list of block texts from the LLM. For each text:  
     * Find the corresponding node in the journal\_tree.  
     * Walk *up* the tree from that node to assemble its "Full Context Chunk" text (all parents \+ the block itself).  
     * Generate the block's original\_id (using its id:: or a content hash, just as in Phase 0). This is for the cleanup step.  
     * Create a "Knowledge Package" object containing the original\_id and the full\_text (with context) and add it to the list.  
4. **Return:** The list of "Knowledge Packages."

### **Phase 2: Candidate Retrieval (RAG)**

**Goal:** Find the Top-K most relevant pages for each Knowledge Package.

**Model:**

* **Embedding:** The *same* local embedding model used in Phase 0\.

#### **Logic**

1. Loop through each "Knowledge Package" from Phase 1\.  
2. **Semantic Search (RAG):**  
   * Generate an embedding for the package's full\_text.  
   * Query the vector database with this embedding to find the Top-K (e.g., 5\) most similar chunks.  
3. **Hinted Search:**  
   * Parse the full\_text for any \[\[Page Links\]\] using a regular expression.  
4. **Aggregate & Gather Context:**  
   * Create a deduplicated set of candidate page names from both search methods.  
   * Augment the "Knowledge Package" object with a candidates list.  
   * Populate this list by storing the page\_name and the specific target\_id and content of the relevant chunks found for that page.  
5. **Return:** The augmented list of "Knowledge Packages."

### **Phase 3 & 3.5: Decision & Rewording**

**Goal:** Decide the action (IGNORE, UPDATE, APPEND) and (if needed) generate the new, clean content.

**Models:**

* **Decider (3.1):** A fast, local, instruction-following LLM.  
* **Reworder (3.2):** A high-quality, generative LLM.

#### **Logic**

1. Initialize a "Write List" (empty list) and a "Processed Journal Blocks" map (empty dictionary).

2. #### **Loop through each "Knowledge Package" and then loop through each of its candidates.**    **Step 3.1: The "Decider" (Local LLM)**

   * Build a prompt for the local LLM, providing it with the "Knowledge Block" (full\_text) and the "Targetable Chunks" (the list of target\_ids and content for the candidate page).  
   * The prompt instructs the LLM to act as a classifier and return a single, structured response (e.g., JSON) with an action and the chosen target\_id.  
   * Valid actions are: IGNORE\_ALREADY\_PRESENT, IGNORE\_IRRELEVANT, UPDATE, APPEND (as child), or APPEND (to root).  
   * Parse the LLM's structured response.

   **Step 3.2: The "Reworder" (Commercial LLM)**

   * #### **If the action from the "Decider" is IGNORE..., stop processing this candidate and move to the next.**

   * If the action is UPDATE or APPEND:  
     * Build a prompt for the high-quality generative LLM.  
     * Provide it *only* with the full\_text of the "Knowledge Package."  
     * Instruct it to rewrite the text into a single, clean, self-contained "Evergreen Block," removing journal context (timestamps, "I think," etc.) while preserving links and core concepts. It must return *only* the new string.  
     * Store this result as new\_content.  
     * **Store the full command:** Add an object to the "Write List" containing the page\_name, the decision (action and target\_id), and the new\_content.  
     * **Store link info for cleanup:** Map the package's original\_id to an object containing the page\_name (this map will be populated with the new\_id in Phase 4).  
3. **Return:** The "Write List" and the "Processed Journal Blocks" map.

### **Phase 4 & 4.5: Execution & Cleanup (Code Only)**

**Goal:** Safely write all changes to disk and mark journal blocks as processed.

#### **Helper Logic: find\_target\_node\_by\_id(ast\_root, target\_id)**

1. This logic must be a function that can traverse the AST provided by the parser module.  
2. It replicates the **Phase 0 Hybrid-ID Generation** logic:  
   * For each node in the tree, it generates its "Full Context Chunk" text.  
   * It checks the node's *own content* for a persistent id::.  
   * If found, the current\_block\_id is that ID.  
   * If not found, the current\_block\_id is the content hash of its "Full Context Chunk" text.  
   * If current\_block\_id matches the target\_id, it returns the node object.  
   * If not found, it returns None.

#### **Main Logic**

1. **Execute Writes:**  
   * Group all operations in the "Write List" by page\_name.  
   * For each page\_name to be modified:  
     * Read the file and parse it into an AST using the round-trip-safe parser.  
     * For each operation in this page's group:  
       * Generate a **new, unique persistent ID** (e.g., a new UUID) for the block to be written.  
       * Combine the new\_content with this new ID (e.g., "{new\_content}\\n id:: {new\_block\_id}").  
       * **If action \== "APPEND" and target\_id \== "root":**  
         * Add a new child node to the AST root with the combined text.  
       * **Else:**  
         * Find the target\_node in the AST using the find\_target\_node\_by\_id logic.  
         * If the node is not found, log an error and continue.  
         * **If action \== "UPDATE":** Set the content of the target\_node to the combined text.  
         * **If action \== "APPEND" (as child):** Add a new child node (with the combined text) to the target\_node.  
       * **Update link map:** Store the new\_block\_id in the "Processed Journal Blocks" map, associating it with the correct original\_id and page\_name.  
     * **Serialize & Write:** Use the parser module to serialize the *entire modified AST* back into a string and overwrite the original page file.  
2. **Cleanup Journal:**  
   * Read and parse the original journal file into an AST.  
   * For each original\_id in the "Processed Journal Blocks" map:  
     * Find the corresponding target\_journal\_node in the journal's AST using the find\_target\_node\_by\_id logic.  
     * Gather all the new link data associated with this original\_id.  
     * Build a list of formatted link strings. The formatting rule for each link is:  
       * Remove .md from the page\_name.  
       * Replace all occurrences of \_\_\_ (three underscores) with /.  
       * Format as: \[\[{formatted\_page\_name}\]\](((new\_id)))  
     * Combine all links into a single string (e.g., comma-separated).  
     * Create the final processed\_text: processed:: {link\_string\_list}  
     * **Add as child:** Add a new child node with the processed\_text to the target\_journal\_node in the AST.  
   * **Serialize & Write:** Use the parser module to serialize the *entire modified journal AST* back to a string and overwrite the original journal file.